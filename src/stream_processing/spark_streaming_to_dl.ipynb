{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "import json\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s:%(funcName)s:%(levelname)s:%(message)s')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('localhost:9000', 'minio_secret_key', 'raw')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################\n",
    "# Parameters & Arguments\n",
    "###############################################\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\")\n",
    "BUCKET_NAME = os.getenv(\"BUCKET_NAME\")\n",
    "MINIO_ENDPOINT, MINIO_ACCESS_KEY, BUCKET_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# PySpark\n",
    "###############################################\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "        Create the Spark Session with suitable configs\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    try: \n",
    "        spark = (SparkSession.builder.config(\"spark.executor.memory\", \"4g\") \\\n",
    "                        .config(\n",
    "                            \"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.hadoop:hadoop-aws:2.8.2\"\n",
    "                        )\n",
    "                        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "                        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "                        .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "                        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "                        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "                        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "                        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "                        .appName(\"Streaming Processing Application\")\n",
    "                        .getOrCreate()\n",
    "        )\n",
    "        \n",
    "        logging.info('Spark session successfully created!')\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the spark session due to exception: {e}\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_dataframe(spark_session):\n",
    "    \"\"\"\n",
    "        Reads the streaming data and creates the initial dataframe accordingly\n",
    "    \"\"\"\n",
    "    try: \n",
    "        df = (spark_session\n",
    "            .readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "            .option(\"subscribe\", \"device.iot.taxi_nyc_time_series\")\n",
    "            # .option(\"startingOffsets\", \"earliest\")\n",
    "            .option(\"failOnDataLoss\", \"false\")\n",
    "            .load())\n",
    "        logging.info(\"Initial dataframe created successfully!\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Initial dataframe could not be created due to exception: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS = ['localhost:9092', 'localhost:9093', 'localhost:9094']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_SERVERS = ['localhost:9092', 'localhost:9093', 'localhost:9094']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'localhost:9092,localhost:9093,localhost:9094'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(BOOTSTRAP_SERVERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static = (spark\n",
    "        .read\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \",\".join(BOOTSTRAP_SERVERS))\n",
    "        .option(\"subscribe\", \"streaming.public.green_trip_raw\")\n",
    "        # .option(\"startingOffsets\", \"earliest\")\n",
    "        .option(\"failOnDataLoss\", \"false\")\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    "        .readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \",\".join(BOOTSTRAP_SERVERS))\n",
    "        .option(\"subscribe\", \"streaming.public.green_trip_raw\")\n",
    "        # .option(\"startingOffsets\", \"earliest\")\n",
    "        .option(\"failOnDataLoss\", \"false\")\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampNTZType, DoubleType, LongType\n",
    "from pyspark.sql.functions import col, from_json, udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./schema_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Define a mapping from type names to PySpark types\n",
    "type_mapping = {\n",
    "    \"IntegerType\": IntegerType(),\n",
    "    \"StringType\": StringType(),\n",
    "    \"TimestampNTZType\": TimestampNTZType(),\n",
    "    \"DoubleType\": DoubleType(),\n",
    "    \"LongType\": LongType()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema based on the configuration file\n",
    "payload_after_schema = StructType([\n",
    "    StructField(field[\"name\"], type_mapping[field[\"type\"]], field[\"nullable\"])\n",
    "    for field in config[\"fields\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dolocationid', IntegerType(), True), StructField('pulocationid', IntegerType(), True), StructField('ratecodeid', DoubleType(), True), StructField('vendorid', IntegerType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('passenger_count', DoubleType(), True), StructField('payment_type', IntegerType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('tpep_dropoff_datetime', LongType(), True), StructField('tpep_pickup_datetime', LongType(), True), StructField('trip_distance', DoubleType(), True)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload_after_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema = StructType([\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"after\", payload_after_schema, True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('payload', StructType([StructField('after', StructType([StructField('dolocationid', IntegerType(), True), StructField('pulocationid', IntegerType(), True), StructField('ratecodeid', DoubleType(), True), StructField('vendorid', IntegerType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('fare_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('passenger_count', DoubleType(), True), StructField('payment_type', IntegerType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('tpep_dropoff_datetime', LongType(), True), StructField('tpep_pickup_datetime', LongType(), True), StructField('trip_distance', DoubleType(), True)]), True)]), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "                .select(from_json(col(\"json\"), data_schema).alias(\"data\")) \\\n",
    "                .select(\"data.payload.after.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = a06088e9-4517-40f5-86cd-dd728c7af118, runId = f3e53613-ecf5-46fe-b45a-2faa4f91c8dc] terminated with exception: Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: C:\\Users\\viett\\AppData\\Local\\Temp\\blockmgr-93b84c72-1b8a-4d23-bffc-8c6962c89754\\0e\njava.nio.file.NoSuchFileException: C:\\Users\\viett\\AppData\\Local\\Temp\\blockmgr-93b84c72-1b8a-4d23-bffc-8c6962c89754\\0e\r\n\tat java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)\r\n\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\r\n\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\r\n\tat java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:509)\r\n\tat java.base/java.nio.file.Files.createDirectory(Files.java:690)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:133)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2076)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1551)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1425)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1924)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:154)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\BigData\\env\\Lib\\site-packages\\pyspark\\sql\\streaming\\query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\BigData\\env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\BigData\\env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mStreamingQueryException\u001b[0m: [STREAM_FAILED] Query [id = a06088e9-4517-40f5-86cd-dd728c7af118, runId = f3e53613-ecf5-46fe-b45a-2faa4f91c8dc] terminated with exception: Job aborted due to stage failure: Task serialization failed: java.nio.file.NoSuchFileException: C:\\Users\\viett\\AppData\\Local\\Temp\\blockmgr-93b84c72-1b8a-4d23-bffc-8c6962c89754\\0e\njava.nio.file.NoSuchFileException: C:\\Users\\viett\\AppData\\Local\\Temp\\blockmgr-93b84c72-1b8a-4d23-bffc-8c6962c89754\\0e\r\n\tat java.base/sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:85)\r\n\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)\r\n\tat java.base/sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:108)\r\n\tat java.base/sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:509)\r\n\tat java.base/java.nio.file.Files.createDirectory(Files.java:690)\r\n\tat org.apache.spark.storage.DiskBlockManager.getFile(DiskBlockManager.scala:108)\r\n\tat org.apache.spark.storage.DiskStore.remove(DiskStore.scala:133)\r\n\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:2076)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1551)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\r\n\tat org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:1425)\r\n\tat org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1924)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:154)\r\n\tat org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:99)\r\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:38)\r\n\tat org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:78)\r\n\tat org.apache.spark.SparkContext.broadcastInternal(SparkContext.scala:1657)\r\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1639)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1585)\r\n\tat org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1337)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3003)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n"
     ]
    }
   ],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dolocationid: int, pulocationid: int, ratecodeid: double, vendorid: int, congestion_surcharge: double, extra: double, fare_amount: double, improvement_surcharge: double, mta_tax: double, passenger_count: double, payment_type: int, tip_amount: double, tolls_amount: double, total_amount: double, tpep_dropoff_datetime: bigint, tpep_pickup_datetime: bigint, trip_distance: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = parsed_df \\\n",
    "    .withColumn(\"tpep_pickup_datetime\", (col(\"tpep_pickup_datetime\") / 1000000).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"tpep_dropoff_datetime\", (col(\"tpep_dropoff_datetime\") / 1000000).cast(\"timestamp\"))\n",
    "\n",
    "parsed_df.createOrReplaceTempView(\"nyc_taxi_view\")\n",
    "\n",
    "df_final = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        * \n",
    "    FROM public.green_trip_raw\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
