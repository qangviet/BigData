{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "import logging\n",
    "import time\n",
    "import dotenv\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from google.cloud import bigquery\n",
    "from pyspark.sql.types import (\n",
    "        StringType,\n",
    "        IntegerType,\n",
    "        LongType,\n",
    "        FloatType,\n",
    "        DoubleType,\n",
    "        BooleanType,\n",
    "        TimestampType,\n",
    "        DateType,\n",
    "        TimestampNTZType\n",
    "    )\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.helpers import load_cfg \n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_cfg(cfg_file):\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML config file\n",
    "    \"\"\"\n",
    "    cfg = None\n",
    "    with open(cfg_file, \"r\") as f:\n",
    "        try:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.minio_utils import MinIOClient\n",
    "\n",
    "from minio import Minio\n",
    "\n",
    "\n",
    "class MinIOClient:\n",
    "    def __init__(self, endpoint_url, access_key, secret_key):\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key\n",
    "\n",
    "    def create_conn(self):\n",
    "        client = Minio(\n",
    "            endpoint=self.endpoint_url,\n",
    "            access_key=self.access_key,\n",
    "            secret_key=self.secret_key,\n",
    "            secure=False,\n",
    "        )\n",
    "        return client\n",
    "\n",
    "    def create_bucket(self, bucket_name):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # Create bucket if not exist\n",
    "        found = client.bucket_exists(bucket_name=bucket_name)\n",
    "        if not found:\n",
    "            client.make_bucket(bucket_name=bucket_name)\n",
    "            print(f\"Bucket {bucket_name} created successfully!\")\n",
    "        else:\n",
    "            print(f\"Bucket {bucket_name} already exists, skip creating!\")\n",
    "\n",
    "    def list_parquet_files(self, bucket_name, prefix=\"\"):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # List all objects in the bucket with the given prefix\n",
    "        objects = client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "        # Filter and collect Parquet file names\n",
    "        parquet_files = [\n",
    "            obj.object_name for obj in objects if obj.object_name.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        return parquet_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s:%(funcName)s:%(levelname)s:%(message)s')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\20241\\Big_data\\MyProject\\src\\batch_processing\n"
     ]
    }
   ],
   "source": [
    "__file__ = os.getcwd()\n",
    "print(__file__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\20241\\\\Big_data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = os.path.dirname(\n",
    "    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    ")\n",
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_FILE = os.path.join(project_root, \"MyProject/config\", \"datalake.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'localhost:9000',\n",
       " 'bucket_name_1': 'raw',\n",
       " 'bucket_name_2': 'processed',\n",
       " 'bucket_name_3': 'sandbox',\n",
       " 'folder_name': 'batch',\n",
       " 'access_key': 'Xs27nx9M4HgPQ5PXZiUE',\n",
       " 'secret_key': '8iifKZlUZh1NRbepsISUMdg1CxlaIC6OSPQk5X59'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = load_cfg(CFG_FILE)\n",
    "datalake_cfg = cfg[\"datalake\"]\n",
    "datalake_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executor_memory': '4g'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG_FILE_SPARK = os.path.join(project_root, \"MyProject/config\", \"spark.yaml\")\n",
    "spark_cfg = load_cfg(CFG_FILE_SPARK)[ \"spark_config\"]\n",
    "spark_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_FILE_BQ = os.path.join(project_root, \"MyProject/config\", \"bigquery.yaml\")\n",
    "bg_cfg = load_cfg(CFG_FILE_BQ)[\"bigquery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data_warehouse', 'bigdata-445102', 'taxi_trips_from_dl')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG_PROJECT_ID = bg_cfg[\"project_id\"]\n",
    "BG_DATASET_ID = bg_cfg[\"dataset_id\"]\n",
    "BG_TABLE_ID_FROM_DL = bg_cfg[\"table_id_1\"]\n",
    "BG_DATASET_ID, BG_PROJECT_ID, BG_TABLE_ID_FROM_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY = spark_cfg[\"executor_memory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\20241\\\\Big_data\\\\MyProject/jars'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JARS_DIR = os.path.join(project_root, \"MyProject/jars\")\n",
    "JARS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ENDPOINT = datalake_cfg[\"endpoint\"]\n",
    "MINIO_ACCESS_KEY = datalake_cfg[\"access_key\"]\n",
    "MINIO_SECRET_KEY = datalake_cfg[\"secret_key\"]\n",
    "BUCKET_NAME_1 = datalake_cfg[\"bucket_name_1\"]\n",
    "BUCKET_NAME_2 = datalake_cfg['bucket_name_2']\n",
    "# BUCKET_NAME_3 = datalake_cfg['bucket_name_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_jars(jars):\n",
    "    \"\"\"\n",
    "    Check if the JAR files exist\n",
    "    \"\"\"\n",
    "    for jar in jars:\n",
    "        if not os.path.exists(jar):\n",
    "            logging.error(f\"JAR file {jar} does not exist!\")\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create a Spark session\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark_jars = [\n",
    "        JARS_DIR + \"/postgresql-42.4.3.jar\", \n",
    "        JARS_DIR + \"/aws-java-sdk-bundle-1.12.262.jar\", \n",
    "        JARS_DIR + \"/hadoop-aws-3.3.4.jar\", \n",
    "        JARS_DIR + \"/spark-bigquery-with-dependencies_2.12-0.30.0.jar\",\n",
    "        JARS_DIR + \"/gcs-connector-hadoop3-latest.jar\"\n",
    "    ]\n",
    "    if not check_jars(spark_jars):\n",
    "        logging.error(\"JAR files do not exist!\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "    try: \n",
    "        spark = (SparkSession.builder.config(\"spark.executor.memory\", MEMORY) \\\n",
    "                        .config(\"spark.jars\", ','.join(spark_jars))\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "                        .appName(\"Batch Processing Application\")\n",
    "                        .getOrCreate()\n",
    "        )\n",
    "        \n",
    "        logging.info('Spark session successfully created!')\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the spark session due to exception: {e}\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_minio_config(spark_context: SparkContext):\n",
    "    \"\"\"\n",
    "        Establist the necessary connection to MinIO\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        logging.info('MinIO configuration is created successfully')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the MinIO configuration due to exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gg_bigquery_config(spark_context: SparkContext):\n",
    "    \"\"\"\n",
    "        Establish the necessary connection to Google BigQ`uery\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.enable\", \"true\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", \"E:/BigData/authencation/account_key_gg_bigquery.json\")\n",
    "        logging.info('Google BigQuery configuration is created successfully')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the Google BigQuery configuration due to exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_to_bigquery_type(spark_type):\n",
    "    \n",
    "    \n",
    "    \"\"\"Ánh xạ kiểu dữ liệu từ Spark sang BigQuery.\"\"\"\n",
    "    if isinstance(spark_type, TimestampNTZType):\n",
    "        return \"DATETIME\"\n",
    "    elif isinstance(spark_type, StringType):\n",
    "        return \"STRING\"\n",
    "    elif isinstance(spark_type, IntegerType):\n",
    "        return \"INT64\"\n",
    "    elif isinstance(spark_type, LongType):\n",
    "        return \"INT64\"\n",
    "    elif isinstance(spark_type, FloatType):\n",
    "        return \"FLOAT64\"\n",
    "    elif isinstance(spark_type, DoubleType):\n",
    "        return \"FLOAT64\"\n",
    "    elif isinstance(spark_type, BooleanType):\n",
    "        return \"BOOL\"\n",
    "    elif isinstance(spark_type, TimestampType):\n",
    "        return \"TIMESTAMP\"\n",
    "    elif isinstance(spark_type, DateType):\n",
    "        return \"DATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_dataframe(df, file_path):\n",
    "    \"\"\"\n",
    "    Processing the dataframe\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F \n",
    "    \n",
    "    df2 = df.withColumn('year', F.year(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"month\", F.date_format(\"pickup_datetime\", \"MMM\")) \\\n",
    "            .withColumn(\"dow\", F.date_format(\"pickup_datetime\", \"EEEE\")) \\\n",
    "            .withColumn(\"pickup_hour\", F.hour(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"pickup_date\", F.to_date(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"pickup_month\", F.month(\"pickup_datetime\"))\n",
    "    \n",
    "    df_final = df2.groupBy(\n",
    "        'year',\n",
    "        'month',\n",
    "        'dow',\n",
    "        F.col('vendorid').alias('vendor_id'),\n",
    "        F.col('ratecodeid').alias('rate_code_id'),\n",
    "        F.col('pulocationid').alias('pickup_location_id'),\n",
    "        F.col('dolocationid').alias('dropoff_location_id'),\n",
    "        F.col('payment_type').alias('payment_type_id'),\n",
    "        'pickup_datetime',\n",
    "        'dropoff_datetime',\n",
    "        'pickup_latitude',\n",
    "        'pickup_longitude',\n",
    "        'dropoff_latitude',\n",
    "        'dropoff_longitude'\n",
    "    ).agg(\n",
    "        F.sum('passenger_count').alias('passenger_count'),\n",
    "        F.sum('trip_distance').alias('trip_distance'),\n",
    "        F.sum('extra').alias('extra'),\n",
    "        F.sum('mta_tax').alias('mta_tax'),\n",
    "        F.sum('fare_amount').alias('fare_amount'),\n",
    "        F.sum('tip_amount').alias('tip_amount'),\n",
    "        F.sum('tolls_amount').alias('tolls_amount'),\n",
    "        F.sum('total_amount').alias('total_amount'),\n",
    "        F.sum('improvement_surcharge').alias('improvement_surcharge'),\n",
    "        F.sum('congestion_surcharge').alias('congestion_surcharge'),\n",
    "    )\n",
    "    \n",
    "    if 'yellow' in file_path:\n",
    "        df_final= df_final.withColumn('service_type', F.lit(1))\n",
    "    elif 'green' in file_path:\n",
    "        df_final= df_final.withColumn('service_type', F.lit(2))\n",
    "        \n",
    "    return df_final                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigquery_schema(spark_schema):\n",
    "    \"\"\"Tạo schema BigQuery từ Spark schema.\"\"\"\n",
    "    bigquery_schema = []\n",
    "    for field in spark_schema.fields:\n",
    "        bigquery_schema.append({\n",
    "            \"name\": field.name,\n",
    "            \"field_type\": spark_to_bigquery_type(field.dataType),\n",
    "            \"mode\": \"NULLABLE\" if field.nullable else \"REQUIRED\"\n",
    "        })\n",
    "    return [bigquery.SchemaField(**field) for field in bigquery_schema] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_bg(project_id, dataset_id, table_id, spark_schema):\n",
    "    \"\"\"\n",
    "    Create a table in Google BigQuery\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    \n",
    "    try:\n",
    "        # Kiểm tra nếu bảng đã tồn tại\n",
    "        client.get_table(table_ref)\n",
    "        print(f\"Bảng '{table_id}' đã tồn tại trong dataset '{dataset_id}'.\")\n",
    "    except Exception as e:\n",
    "        if \"Not found\" in str(e):\n",
    "            print(f\"Bảng '{table_id}' không tồn tại. Đang tạo bảng mới...\")\n",
    "\n",
    "            # Tạo schema BigQuery từ danh sách dictionary\n",
    "            bigquery_schema = create_bigquery_schema(spark_schema)\n",
    "            # Định nghĩa bảng mới\n",
    "            table = bigquery.Table(table_ref, schema=bigquery_schema)\n",
    "            # Tạo bảng trong BigQuery\n",
    "            client.create_table(table)\n",
    "            print(f\"Đã tạo bảng '{table_id}' thành công trong dataset '{dataset_id}'.\")\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 02:21:11,119:create_spark_session:INFO:Spark session successfully created!\n",
      "2024-12-20 02:21:11,121:load_minio_config:INFO:MinIO configuration is created successfully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark = create_spark_session()\n",
    "    load_minio_config(spark.sparkContext)\n",
    "\n",
    "    client = MinIOClient(\n",
    "        endpoint_url=MINIO_ENDPOINT,\n",
    "        access_key=MINIO_ACCESS_KEY,\n",
    "        secret_key=MINIO_SECRET_KEY\n",
    "    )\n",
    "    YEAR = '2024'\n",
    "    files = client.list_parquet_files(BUCKET_NAME_2, prefix=YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024/green_tripdata_2024-01.parquet', '2024/yellow_tripdata_2024-01.parquet']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = client.list_parquet_files(BUCKET_NAME_2, prefix=YEAR)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 02:21:11,208:<module>:INFO:Reading parquet file: 2024/yellow_tripdata_2024-01.parquet\n"
     ]
    }
   ],
   "source": [
    "path = f\"s3a://{BUCKET_NAME_2}/{file}\"\n",
    "logging.info(f\"Reading parquet file: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = processing_dataframe(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bảng 'taxi_trips_from_dl' đã tồn tại trong dataset 'data_warehouse'.\n"
     ]
    }
   ],
   "source": [
    "create_table_bg(BG_PROJECT_ID, BG_DATASET_ID, BG_TABLE_ID_FROM_DL, df_final.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 02:21:18,543:load_gg_bigquery_config:INFO:Google BigQuery configuration is created successfully\n"
     ]
    }
   ],
   "source": [
    "load_gg_bigquery_config(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.limit(100).write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{BG_PROJECT_ID}:{BG_DATASET_ID}.{BG_TABLE_ID_FROM_DL}\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"temp_for_bq\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 17:58:13,223:create_spark_session:INFO:Spark session successfully created!\n",
      "2024-12-19 17:58:13,407:load_minio_config:INFO:MinIO configuration is created successfully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    spark = create_spark_session()\n",
    "    load_minio_config(spark.sparkContext)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
