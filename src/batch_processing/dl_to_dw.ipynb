{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "import logging\n",
    "import time\n",
    "import dotenv\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from google.cloud import bigquery\n",
    "from pyspark.sql.types import (\n",
    "        StringType,\n",
    "        IntegerType,\n",
    "        LongType,\n",
    "        FloatType,\n",
    "        DoubleType,\n",
    "        BooleanType,\n",
    "        TimestampType,\n",
    "        DateType,\n",
    "        TimestampNTZType\n",
    "    )\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\01\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\02\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\03\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\04\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\05\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\06\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\07\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\08\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\09\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\10\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\11\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\12\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\13\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\14\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\15\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\16\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\17\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\18\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\19\\transform2.parquet\n",
      "Đã xóa: D:/20241/Big_data/MyProject/data/transform_data/2024/02\\20\\transform2.parquet\n",
      "Thư mục D:/20241/Big_data/MyProject/data/transform_data/2024/02\\30 không tồn tại!\n",
      "Thư mục D:/20241/Big_data/MyProject/data/transform_data/2024/02\\31 không tồn tại!\n"
     ]
    }
   ],
   "source": [
    "folder = 'D:/20241/Big_data/MyProject/data/transform_data/2024/02'\n",
    "import os\n",
    "for i in range (1, 32):\n",
    "    sub = os.path.join(folder, f'{i:02}')\n",
    "    file_name = 'transform2.parquet'\n",
    "    if os.path.isdir(sub):\n",
    "            # Lặp qua tất cả các file trong thư mục\n",
    "            for root, dirs, files in os.walk(sub):\n",
    "                for file in files:\n",
    "                    # Nếu tên file trùng với file_name, xóa nó\n",
    "                    if file == file_name:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        try:\n",
    "                            os.remove(file_path)\n",
    "                            print(f\"Đã xóa: {file_path}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Không thể xóa {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Thư mục {sub} không tồn tại!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.helpers import load_cfg \n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_cfg(cfg_file):\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML config file\n",
    "    \"\"\"\n",
    "    cfg = None\n",
    "    with open(cfg_file, \"r\") as f:\n",
    "        try:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.minio_utils import MinIOClient\n",
    "\n",
    "from minio import Minio\n",
    "\n",
    "\n",
    "class MinIOClient:\n",
    "    def __init__(self, endpoint_url, access_key, secret_key):\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key\n",
    "\n",
    "    def create_conn(self):\n",
    "        client = Minio(\n",
    "            endpoint=self.endpoint_url,\n",
    "            access_key=self.access_key,\n",
    "            secret_key=self.secret_key,\n",
    "            secure=False,\n",
    "        )\n",
    "        return client\n",
    "\n",
    "    def create_bucket(self, bucket_name):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # Create bucket if not exist\n",
    "        found = client.bucket_exists(bucket_name=bucket_name)\n",
    "        if not found:\n",
    "            client.make_bucket(bucket_name=bucket_name)\n",
    "            print(f\"Bucket {bucket_name} created successfully!\")\n",
    "        else:\n",
    "            print(f\"Bucket {bucket_name} already exists, skip creating!\")\n",
    "\n",
    "    def list_parquet_files(self, bucket_name, prefix=\"\"):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # List all objects in the bucket with the given prefix\n",
    "        objects = client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "        # Filter and collect Parquet file names\n",
    "        parquet_files = [\n",
    "            obj.object_name for obj in objects if obj.object_name.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        return parquet_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s:%(funcName)s:%(levelname)s:%(message)s')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\BigData_2\\MyProject\\src\\batch_processing\n"
     ]
    }
   ],
   "source": [
    "__file__ = os.getcwd()\n",
    "print(__file__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\BigData_2\\\\MyProject'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = os.path.dirname(\n",
    "    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    ")\n",
    "project_root = os.path.join(project_root, 'MyProject')\n",
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_FILE = os.path.join(project_root, \"config\", \"datalake.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'localhost:9000',\n",
       " 'bucket_name_1': 'raw',\n",
       " 'bucket_name_2': 'processed',\n",
       " 'bucket_name_3': 'sandbox',\n",
       " 'folder_name': 'batch',\n",
       " 'access_key': 'Xs27nx9M4HgPQ5PXZiUE',\n",
       " 'secret_key': '8iifKZlUZh1NRbepsISUMdg1CxlaIC6OSPQk5X59'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = load_cfg(CFG_FILE)\n",
    "datalake_cfg = cfg[\"datalake\"]\n",
    "datalake_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executor_memory': '4g'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG_FILE_SPARK = os.path.join(project_root, \"config\", \"spark.yaml\")\n",
    "spark_cfg = load_cfg(CFG_FILE_SPARK)[ \"spark_config\"]\n",
    "spark_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_FILE_BQ = os.path.join(project_root, \"config\", \"bigquery.yaml\")\n",
    "bg_cfg = load_cfg(CFG_FILE_BQ)[\"bigquery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data_warehouse', 'bigdata-445102', 'taxi_trips_from_dl')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BG_PROJECT_ID = bg_cfg[\"project_id\"]\n",
    "BG_DATASET_ID = bg_cfg[\"dataset_id\"]\n",
    "BG_TABLE_ID_FROM_DL = bg_cfg[\"table_id_1\"]\n",
    "BG_DATASET_ID, BG_PROJECT_ID, BG_TABLE_ID_FROM_DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY = spark_cfg[\"executor_memory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\BigData_2\\\\MyProject\\\\jars'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JARS_DIR = os.path.join(project_root, \"jars\")\n",
    "JARS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ENDPOINT = datalake_cfg[\"endpoint\"]\n",
    "MINIO_ACCESS_KEY = datalake_cfg[\"access_key\"]\n",
    "MINIO_SECRET_KEY = datalake_cfg[\"secret_key\"]\n",
    "BUCKET_NAME_1 = datalake_cfg[\"bucket_name_1\"]\n",
    "BUCKET_NAME_2 = datalake_cfg['bucket_name_2']\n",
    "# BUCKET_NAME_3 = datalake_cfg['bucket_name_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_jars(jars):\n",
    "    \"\"\"\n",
    "    Check if the JAR files exist\n",
    "    \"\"\"\n",
    "    for jar in jars:\n",
    "        if not os.path.exists(jar):\n",
    "            logging.error(f\"JAR file {jar} does not exist!\")\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create a Spark session\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark_jars = [\n",
    "        JARS_DIR + \"/postgresql-42.4.3.jar\", \n",
    "        JARS_DIR + \"/aws-java-sdk-bundle-1.12.262.jar\", \n",
    "        JARS_DIR + \"/hadoop-aws-3.3.4.jar\", \n",
    "        JARS_DIR + \"/spark-bigquery-with-dependencies_2.12-0.30.0.jar\",\n",
    "        JARS_DIR + \"/gcs-connector-hadoop3-latest.jar\"\n",
    "    ]\n",
    "    if not check_jars(spark_jars):\n",
    "        logging.error(\"JAR files do not exist!\")\n",
    "        return None\n",
    "    \n",
    "    spark = None\n",
    "    try: \n",
    "        spark = (SparkSession.builder.config(\"spark.executor.memory\", MEMORY) \\\n",
    "                        .config(\"spark.jars\", ','.join(spark_jars))\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "                        .appName(\"Batch Processing Application\")\n",
    "                        .getOrCreate()\n",
    "        )\n",
    "        \n",
    "        logging.info('Spark session successfully created!')\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the spark session due to exception: {e}\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_minio_config(spark_context: SparkContext):\n",
    "    \"\"\"\n",
    "        Establist the necessary connection to MinIO\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        logging.info('MinIO configuration is created successfully')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the MinIO configuration due to exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gg_bigquery_config(spark_context: SparkContext):\n",
    "    \"\"\"\n",
    "        Establish the necessary connection to Google BigQ`uery\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.enable\", \"true\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", \"E:/BigData/authencation/account_key_gg_bigquery.json\")\n",
    "        logging.info('Google BigQuery configuration is created successfully')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the Google BigQuery configuration due to exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_to_bigquery_type(spark_type):\n",
    "    \n",
    "    \n",
    "    \"\"\"Ánh xạ kiểu dữ liệu từ Spark sang BigQuery.\"\"\"\n",
    "    if isinstance(spark_type, TimestampNTZType):\n",
    "        return \"DATETIME\"\n",
    "    elif isinstance(spark_type, StringType):\n",
    "        return \"STRING\"\n",
    "    elif isinstance(spark_type, IntegerType):\n",
    "        return \"INT64\"\n",
    "    elif isinstance(spark_type, LongType):\n",
    "        return \"INT64\"\n",
    "    elif isinstance(spark_type, FloatType):\n",
    "        return \"FLOAT64\"\n",
    "    elif isinstance(spark_type, DoubleType):\n",
    "        return \"FLOAT64\"\n",
    "    elif isinstance(spark_type, BooleanType):\n",
    "        return \"BOOL\"\n",
    "    elif isinstance(spark_type, TimestampType):\n",
    "        return \"TIMESTAMP\"\n",
    "    elif isinstance(spark_type, DateType):\n",
    "        return \"DATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_dataframe(df, file_path):\n",
    "    \"\"\"\n",
    "    Processing the dataframe\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F \n",
    "    \n",
    "    df2 = df.withColumn('year', F.year(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"month\", F.date_format(\"pickup_datetime\", \"MMM\")) \\\n",
    "            .withColumn(\"dow\", F.date_format(\"pickup_datetime\", \"EEEE\")) \\\n",
    "            .withColumn(\"pickup_hour\", F.hour(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"pickup_date\", F.to_date(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"pickup_month\", F.month(\"pickup_datetime\"))\n",
    "    \n",
    "    df_final = df2.groupBy(\n",
    "        'year',\n",
    "        'month',\n",
    "        'dow',\n",
    "        F.col('vendorid').alias('vendor_id'),\n",
    "        F.col('ratecodeid').alias('rate_code_id'),\n",
    "        F.col('pulocationid').alias('pickup_location_id'),\n",
    "        F.col('dolocationid').alias('dropoff_location_id'),\n",
    "        F.col('payment_type').alias('payment_type_id'),\n",
    "        'pickup_datetime',\n",
    "        'dropoff_datetime',\n",
    "        'pickup_latitude',\n",
    "        'pickup_longitude',\n",
    "        'dropoff_latitude',\n",
    "        'dropoff_longitude'\n",
    "    ).agg(\n",
    "        F.sum('passenger_count').alias('passenger_count'),\n",
    "        F.sum('trip_distance').alias('trip_distance'),\n",
    "        F.sum('extra').alias('extra'),\n",
    "        F.sum('mta_tax').alias('mta_tax'),\n",
    "        F.sum('fare_amount').alias('fare_amount'),\n",
    "        F.sum('tip_amount').alias('tip_amount'),\n",
    "        F.sum('tolls_amount').alias('tolls_amount'),\n",
    "        F.sum('total_amount').alias('total_amount'),\n",
    "        F.sum('improvement_surcharge').alias('improvement_surcharge'),\n",
    "        F.sum('congestion_surcharge').alias('congestion_surcharge'),\n",
    "    )\n",
    "    \n",
    "    if 'yellow' in file_path:\n",
    "        df_final= df_final.withColumn('service_type', F.lit(1))\n",
    "    elif 'green' in file_path:\n",
    "        df_final= df_final.withColumn('service_type', F.lit(2))\n",
    "        \n",
    "    return df_final                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigquery_schema(spark_schema):\n",
    "    \"\"\"Tạo schema BigQuery từ Spark schema.\"\"\"\n",
    "    bigquery_schema = []\n",
    "    for field in spark_schema.fields:\n",
    "        bigquery_schema.append({\n",
    "            \"name\": field.name,\n",
    "            \"field_type\": spark_to_bigquery_type(field.dataType),\n",
    "            \"mode\": \"NULLABLE\" if field.nullable else \"REQUIRED\"\n",
    "        })\n",
    "    return [bigquery.SchemaField(**field) for field in bigquery_schema] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_bg(project_id, dataset_id, table_id, spark_schema):\n",
    "    \"\"\"\n",
    "    Create a table in Google BigQuery\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    \n",
    "    try:\n",
    "        # Kiểm tra nếu bảng đã tồn tại\n",
    "        client.get_table(table_ref)\n",
    "        print(f\"Bảng '{table_id}' đã tồn tại trong dataset '{dataset_id}'.\")\n",
    "    except Exception as e:\n",
    "        if \"Not found\" in str(e):\n",
    "            print(f\"Bảng '{table_id}' không tồn tại. Đang tạo bảng mới...\")\n",
    "\n",
    "            # Tạo schema BigQuery từ danh sách dictionary\n",
    "            bigquery_schema = create_bigquery_schema(spark_schema)\n",
    "            # Định nghĩa bảng mới\n",
    "            table = bigquery.Table(table_ref, schema=bigquery_schema)\n",
    "            # Tạo bảng trong BigQuery\n",
    "            client.create_table(table)\n",
    "            print(f\"Đã tạo bảng '{table_id}' thành công trong dataset '{dataset_id}'.\")\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 22:07:47,613:create_spark_session:INFO:Spark session successfully created!\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 22:07:49,659:load_minio_config:INFO:MinIO configuration is created successfully\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "load_minio_config(spark.sparkContext)\n",
    "\n",
    "client = MinIOClient(\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY\n",
    ")\n",
    "YEAR = '2024'\n",
    "files = client.list_parquet_files(BUCKET_NAME_2, prefix=YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024/01/green_tripdata_2024-01.parquet',\n",
       " '2024/01/yellow_tripdata_2024-01.parquet',\n",
       " '2024/02/green_tripdata_2024-02.parquet',\n",
       " '2024/02/yellow_tripdata_2024-02.parquet',\n",
       " '2024/03/green_tripdata_2024-03.parquet',\n",
       " '2024/03/yellow_tripdata_2024-03.parquet',\n",
       " '2024/04/green_tripdata_2024-04.parquet',\n",
       " '2024/04/yellow_tripdata_2024-04.parquet',\n",
       " '2024/05/green_tripdata_2024-05.parquet',\n",
       " '2024/05/yellow_tripdata_2024-05.parquet',\n",
       " '2024/06/green_tripdata_2024-06.parquet',\n",
       " '2024/06/yellow_tripdata_2024-06.parquet',\n",
       " '2024/07/green_tripdata_2024-07.parquet',\n",
       " '2024/07/yellow_tripdata_2024-07.parquet',\n",
       " '2024/08/green_tripdata_2024-08.parquet',\n",
       " '2024/08/yellow_tripdata_2024-08.parquet',\n",
       " '2024/09/green_tripdata_2024-09.parquet',\n",
       " '2024/09/yellow_tripdata_2024-09.parquet',\n",
       " '2024/10/green_tripdata_2024-10.parquet',\n",
       " '2024/10/yellow_tripdata_2024-10.parquet']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = client.list_parquet_files(BUCKET_NAME_2, prefix=YEAR)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 22:08:04,792:<module>:INFO:Reading parquet file: 2024/01/yellow_tripdata_2024-01.parquet\n"
     ]
    }
   ],
   "source": [
    "path = f\"s3a://{BUCKET_NAME_2}/{file}\"\n",
    "logging.info(f\"Reading parquet file: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = processing_dataframe(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bảng 'test_2024' không tồn tại. Đang tạo bảng mới...\n",
      "Đã tạo bảng 'test_2024' thành công trong dataset 'data_warehouse'.\n"
     ]
    }
   ],
   "source": [
    "create_table_bg(BG_PROJECT_ID, BG_DATASET_ID, \"test_2024\", df_final.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 22:08:49,314:load_gg_bigquery_config:INFO:Google BigQuery configuration is created successfully\n"
     ]
    }
   ],
   "source": [
    "load_gg_bigquery_config(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.limit(50).write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"table\", f\"{BG_PROJECT_ID}:{BG_DATASET_ID}.test_2024\") \\\n",
    "    .option(\"temporaryGcsBucket\", \"temp_for_bq\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2 = f\"s3a://{BUCKET_NAME_2}/data/green/2024/01/02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.read.format('parquet').load(path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+----------------+------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+----------+------------+------------+-------------+--------+\n",
      "|congestion_surcharge|dolocationid|   dropoff_datetime|dropoff_latitude| dropoff_longitude|extra|fare_amount|            id|id_customer|improvement_surcharge|mta_tax|passenger_count|payment_type|    pickup_datetime|pickup_latitude|  pickup_longitude|pulocationid|rate|ratecodeid|tip_amount|tolls_amount|total_amount|trip_distance|vendorid|\n",
      "+--------------------+------------+-------------------+----------------+------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+----------+------------+------------+-------------+--------+\n",
      "|                 0.0|          82|2024-01-02 07:53:54|      41.8994745|       -87.9403418|  1.0|       17.0|20240100001113|     537305|                  1.0|    0.5|            1.0|           1|2024-01-02 07:37:01|     41.8994745|       -87.9403418|          83| 1.2|       1.0|      4.88|         0.0|       24.38|         2.55|       2|\n",
      "|                 0.0|          10|2024-01-02 07:30:02|      42.2038609|       -83.1731182|  1.0|       11.4|20240100001114|     167952|                  1.0|    0.5|            1.0|           1|2024-01-02 07:21:06|     18.1850507|       -77.3947693|         130| 1.5|       1.0|      3.48|         0.0|       17.38|         1.87|       2|\n",
      "|                 0.0|         122|2024-01-02 07:58:23|      35.6674547|      -119.1912178|  1.0|       17.0|20240100001115|     822095|                  1.0|    0.5|            1.0|           1|2024-01-02 07:44:10|     18.1850507|       -77.3947693|         130| 3.6|       1.0|       2.1|         0.0|        21.6|         3.14|       2|\n",
      "|                 0.0|         136|2024-01-02 08:06:08|      40.8725702|       -73.9026619|  1.0|       14.2|20240100001117|     133721|                  1.0|    0.5|            5.0|           2|2024-01-02 07:56:05|     35.2557439|       -80.8602626|         243| 1.7|       1.0|       0.0|         0.0|        16.7|          2.6|       2|\n",
      "|                 0.0|          70|2024-01-02 07:16:05|      40.7612123|       -73.8651358|  1.0|       16.3|20240100001123|     861649|                  1.0|    0.5|            1.0|           1|2024-01-02 07:07:24|     40.7556561|       -73.8857755|         129| 3.2|       1.0|       2.0|         0.0|        20.8|         3.45|       2|\n",
      "|                 0.0|         198|2024-01-02 07:15:25|      40.9796683|       -74.1194403|  0.0|       35.0|20240100001124|     154898|                  1.0|    0.0|            2.0|           1|2024-01-02 07:15:21|     40.9796683|       -74.1194403|         198| 4.9|       5.0|       0.0|         0.0|        36.0|          0.0|       2|\n",
      "|                 0.0|         157|2024-01-02 08:00:17|       40.723158|        -73.912637|  0.0|       75.0|20240100001125|     459983|                  1.0|    0.0|            1.0|           1|2024-01-02 07:59:46|      40.723158|        -73.912637|         157| 2.3|       5.0|      15.2|         0.0|        91.2|         0.13|       2|\n",
      "|                 0.0|         258|2024-01-02 08:08:10|      42.1370519|       -83.2451066|  1.0|       26.1|20240100001126|     577583|                  1.0|    0.5|            1.0|           2|2024-01-02 07:26:42|     40.7556561|       -73.8857755|         129| 4.4|       1.0|       0.0|         0.0|        28.6|         4.84|       2|\n",
      "|                 0.0|         264|2024-01-02 07:49:10|      39.5158825|       -116.853722|  0.0|       50.0|20240100001127|     677989|                  0.0|    0.0|            1.0|           1|2024-01-02 07:48:10|     39.5158825|       -116.853722|         264| 5.0|       5.0|      8.08|         0.0|       58.08|          0.0|       2|\n",
      "|                 0.0|         157|2024-01-02 07:25:26|       40.723158|        -73.912637|  1.0|       11.4|20240100001128|     901710|                  1.0|    0.5|            5.0|           2|2024-01-02 07:15:57|     37.4299388|       -122.253855|         260| 2.4|       1.0|       0.0|         0.0|        13.9|         1.94|       2|\n",
      "|                 0.0|          82|2024-01-02 07:39:42|      41.8994745|       -87.9403418|  1.0|        5.1|20240100001129|     247723|                  1.0|    1.5|            1.0|           3|2024-01-02 07:36:12|     41.8994745|       -87.9403418|          82| 5.0|       1.0|       0.0|         0.0|         7.6|          0.4|       1|\n",
      "|                 0.0|          41|2024-01-02 18:45:57|     22.92109965|-83.19403675174992|  0.0|       12.8|20240100001627|     415571|                  1.0|    0.5|            1.0|           2|2024-01-02 18:31:27|     41.9823426|       -87.8070729|          74| 2.4|       1.0|       0.0|         0.0|        14.3|         1.47|       2|\n",
      "|                 0.0|         151|2024-01-02 18:10:54|      40.7970037|        -73.969718|  0.0|       10.0|20240100001629|     921375|                  1.0|    0.5|            1.0|           1|2024-01-02 18:03:23|          40.81|          -73.9625|         166| 4.0|       1.0|      3.45|         0.0|       14.95|         1.43|       2|\n",
      "|                2.75|         234|2024-01-02 19:16:37|      40.7360717|       -73.9901888|  0.0|       32.4|20240100001630|     443307|                  1.0|    0.5|            1.0|           1|2024-01-02 18:43:29|    22.92109965|-83.19403675174992|          41| 1.1|       1.0|      7.33|         0.0|       43.98|         5.52|       2|\n",
      "|                 0.0|         145|2024-01-02 18:36:26|      40.7415095|       -73.9569751|  0.0|       17.0|20240100001590|     765682|                  1.0|    0.5|            1.0|           2|2024-01-02 18:21:19|     46.1882007|      -123.8319802|           7| 5.0|       1.0|       0.0|         0.0|        18.5|         2.94|       2|\n",
      "|                 0.0|         138|2024-01-02 19:09:07|      40.7757145|-73.87336398511545|  5.0|       28.2|20240100001591|     416082|                  1.0|    1.5|            2.0|           1|2024-01-02 18:50:57|    22.92109965|-83.19403675174992|          41| 2.9|       1.0|       8.3|        6.94|       49.94|          6.0|       1|\n",
      "|                 0.0|          28|2024-01-02 18:29:23|      38.2781258|       -85.5930164|  0.0|       22.6|20240100001592|     831619|                  1.0|    0.5|            1.0|           2|2024-01-02 18:12:41|     41.8994745|       -87.9403418|          82| 1.3|       1.0|       0.0|         0.0|        24.1|         4.69|       2|\n",
      "|                2.75|         137|2024-01-02 17:58:28|      40.7395463|       -73.9770832|  0.0|       24.7|20240100001531|     340983|                  1.0|    0.5|            1.0|           1|2024-01-02 17:43:41|     41.9823426|       -87.8070729|          74| 1.7|       1.0|       2.0|         0.0|       30.95|         6.06|       2|\n",
      "|                 0.0|          74|2024-01-02 16:36:13|      41.9823426|       -87.8070729|  0.0|       10.7|20240100001532|     407684|                  1.0|    0.5|            2.0|           2|2024-01-02 16:27:26|     40.8089897|       -73.9229147|         168| 3.4|       1.0|       0.0|         0.0|        12.2|         1.69|       2|\n",
      "|                2.75|         239|2024-01-02 17:32:50|      -35.015787|        138.636155|  0.0|       17.7|20240100001533|     832628|                  1.0|    0.5|            2.0|           1|2024-01-02 17:16:16|     41.9823426|       -87.8070729|          74| 5.0|       1.0|       3.0|         0.0|       24.95|         3.12|       2|\n",
      "+--------------------+------------+-------------------+----------------+------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+----------+------------+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- dolocationid: integer (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- id_customer: long (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pulocationid: integer (nullable = true)\n",
      " |-- rate: double (nullable = true)\n",
      " |-- ratecodeid: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- vendorid: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_3 = spark.read.csv(\"s3a://raw/taxi_lookup.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 17:58:13,223:create_spark_session:INFO:Spark session successfully created!\n",
      "2024-12-19 17:58:13,407:load_minio_config:INFO:MinIO configuration is created successfully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    spark = create_spark_session()\n",
    "    load_minio_config(spark.sparkContext)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
