{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import traceback\n",
    "import logging\n",
    "import time\n",
    "import dotenv\n",
    "from pyspark import SparkConf, SparkContext\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.helpers import load_cfg \n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_cfg(cfg_file):\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML config file\n",
    "    \"\"\"\n",
    "    cfg = None\n",
    "    with open(cfg_file, \"r\") as f:\n",
    "        try:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.minio_utils import MinIOClient\n",
    "\n",
    "from minio import Minio\n",
    "\n",
    "\n",
    "class MinIOClient:\n",
    "    def __init__(self, endpoint_url, access_key, secret_key):\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key\n",
    "\n",
    "    def create_conn(self):\n",
    "        client = Minio(\n",
    "            endpoint=self.endpoint_url,\n",
    "            access_key=self.access_key,\n",
    "            secret_key=self.secret_key,\n",
    "            secure=False,\n",
    "        )\n",
    "        return client\n",
    "\n",
    "    def create_bucket(self, bucket_name):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # Create bucket if not exist\n",
    "        found = client.bucket_exists(bucket_name=bucket_name)\n",
    "        if not found:\n",
    "            client.make_bucket(bucket_name=bucket_name)\n",
    "            print(f\"Bucket {bucket_name} created successfully!\")\n",
    "        else:\n",
    "            print(f\"Bucket {bucket_name} already exists, skip creating!\")\n",
    "\n",
    "    def list_parquet_files(self, bucket_name, prefix=\"\"):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # List all objects in the bucket with the given prefix\n",
    "        objects = client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "        # Filter and collect Parquet file names\n",
    "        parquet_files = [\n",
    "            obj.object_name for obj in objects if obj.object_name.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        return parquet_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s:%(funcName)s:%(levelname)s:%(message)s')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSTGRES_USER = os.getenv(\"DB_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "POSTGRES_HOST = os.getenv(\"DB_HOST\")\n",
    "POSTGRES_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_STAGING_TABLE = os.getenv(\"DB_STAGING_TABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\BigData\\MyProject\\src\\batch_processing\n"
     ]
    }
   ],
   "source": [
    "__file__ = os.getcwd()\n",
    "print(__file__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\BigData'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = os.path.dirname(\n",
    "    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    ")\n",
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_FILE = os.path.join(project_root, \"MyProject/config\", \"datalake.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'localhost:9000',\n",
       " 'bucket_name_1': 'raw',\n",
       " 'bucket_name_2': 'processed',\n",
       " 'bucket_name_3': 'sandbox',\n",
       " 'folder_name': 'batch',\n",
       " 'access_key': '0VQBMtMhycuIrat2ivLH',\n",
       " 'secret_key': 'xozBRG1AkxBkEnwN3JePy1BhhvHQGtE1sCAEmZeI'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = load_cfg(CFG_FILE)\n",
    "datalake_cfg = cfg[\"datalake\"]\n",
    "datalake_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executor_memory': '4g'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG_FILE_SPARK = os.path.join(project_root, \"MyProject/config\", \"spark.yaml\")\n",
    "spark_cfg = load_cfg(CFG_FILE_SPARK)[ \"spark_config\"]\n",
    "spark_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY = spark_cfg[\"executor_memory\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ENDPOINT = datalake_cfg[\"endpoint\"]\n",
    "MINIO_ACCESS_KEY = datalake_cfg[\"access_key\"]\n",
    "MINIO_SECRET_KEY = datalake_cfg[\"secret_key\"]\n",
    "BUCKET_NAME_2 = datalake_cfg['bucket_name_2']\n",
    "BUCKET_NAME_3 = datalake_cfg['bucket_name_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create a Spark session\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    try: \n",
    "        spark = (SparkSession.builder.config(\"spark.executor.memory\", MEMORY) \\\n",
    "                        .config(\n",
    "                            \"spark.jars\", \n",
    "                            \"jars/postgresql-42.4.3.jar,jars/aws-java-sdk-bundle-1.12.262.jar,jars/hadoop-aws-3.3.4.jar\",\n",
    "                        )\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "                        .appName(\"Batch Processing Application\")\n",
    "                        .getOrCreate()\n",
    "        )\n",
    "        \n",
    "        logging.info('Spark session successfully created!')\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the spark session due to exception: {e}\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_minio_config(spark_context: SparkContext):\n",
    "    \"\"\"\n",
    "        Establist the necessary connection to MinIO\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        logging.info('MinIO configuration is created successfully')\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stderr)\n",
    "        logging.error(f\"Couldn't create the MinIO configuration due to exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_dataframe(df, file_path):\n",
    "    \"\"\"\n",
    "    Processing the dataframe\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F \n",
    "    \n",
    "    df2 = df.withColumn('year', F.year(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"month\", F.date_format(\"pickup_datetime\", \"MMM\")) \\\n",
    "            .withColumn(\"dow\", F.date_format(\"pickup_datetime\", \"EEEE\"))\n",
    "            \n",
    "    df_final = df2.groupBy(\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"dow\",\n",
    "        F.col(\"vendor_id\").alias(\"vendor_id\"),\n",
    "        F.``\n",
    "    )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
