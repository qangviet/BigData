{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.helpers import load_cfg \n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_cfg(cfg_file):\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML config file\n",
    "    \"\"\"\n",
    "    cfg = None\n",
    "    with open(cfg_file, \"r\") as f:\n",
    "        try:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.minio_utils import MinIOClient\n",
    "\n",
    "from minio import Minio\n",
    "\n",
    "\n",
    "class MinIOClient:\n",
    "    def __init__(self, endpoint_url, access_key, secret_key):\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key\n",
    "\n",
    "    def create_conn(self):\n",
    "        client = Minio(\n",
    "            endpoint=self.endpoint_url,\n",
    "            access_key=self.access_key,\n",
    "            secret_key=self.secret_key,\n",
    "            secure=False,\n",
    "        )\n",
    "        return client\n",
    "\n",
    "    def create_bucket(self, bucket_name):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # Create bucket if not exist\n",
    "        found = client.bucket_exists(bucket_name=bucket_name)\n",
    "        if not found:\n",
    "            client.make_bucket(bucket_name=bucket_name)\n",
    "            print(f\"Bucket {bucket_name} created successfully!\")\n",
    "        else:\n",
    "            print(f\"Bucket {bucket_name} already exists, skip creating!\")\n",
    "\n",
    "    def list_parquet_files(self, bucket_name, prefix=\"\"):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # List all objects in the bucket with the given prefix\n",
    "        objects = client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "        # Filter and collect Parquet file names\n",
    "        parquet_files = [\n",
    "            obj.object_name for obj in objects if obj.object_name.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        return parquet_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s:%(funcName)s:%(levelname)s:%(message)s')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\BigData_2\\MyProject\\src\\batch_processing\n"
     ]
    }
   ],
   "source": [
    "__file__ = os.getcwd()\n",
    "print(__file__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\BigData_2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = os.path.dirname(\n",
    "    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    ")\n",
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_FILE = os.path.join(project_root, \"MyProject/config\", \"datalake.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'localhost:9000',\n",
       " 'bucket_name_1': 'raw',\n",
       " 'bucket_name_2': 'processed',\n",
       " 'bucket_name_3': 'sandbox',\n",
       " 'folder_name': 'batch',\n",
       " 'access_key': 'Xs27nx9M4HgPQ5PXZiUE',\n",
       " 'secret_key': '8iifKZlUZh1NRbepsISUMdg1CxlaIC6OSPQk5X59'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = load_cfg(CFG_FILE)\n",
    "datalake_cfg = cfg[\"datalake\"]\n",
    "datalake_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ENDPOINT = datalake_cfg[\"endpoint\"]\n",
    "MINIO_ACCESS_KEY = datalake_cfg[\"access_key\"]\n",
    "MINIO_SECRET_KEY = datalake_cfg[\"secret_key\"]\n",
    "BUCKET_NAME_2 = datalake_cfg['bucket_name_2']\n",
    "BUCKET_NAME_3 = datalake_cfg['bucket_name_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('localhost:9000',\n",
       " 'Xs27nx9M4HgPQ5PXZiUE',\n",
       " '8iifKZlUZh1NRbepsISUMdg1CxlaIC6OSPQk5X59',\n",
       " 'processed',\n",
       " 'sandbox')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MINIO_ENDPOINT, MINIO_ACCESS_KEY, MINIO_SECRET_KEY, BUCKET_NAME_2, BUCKET_NAME_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\BigData_2\\\\MyProject/jars'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JARS_DIR = os.path.join(project_root, \"MyProject/jars\")\n",
    "JARS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jars =  [ JARS_DIR + \"/hadoop-aws-3.3.4.jar\", JARS_DIR + \"/aws-java-sdk-bundle-1.12.262.jar\", \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "','.join(jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# PySpark\n",
    "###############################################\n",
    "\n",
    "def delta_convert(endpoint_url, access_key, secret_key):\n",
    "    \"\"\"\n",
    "        Convert parquet file to delta format\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    from delta.pip_utils import configure_spark_with_delta_pip\n",
    "    \n",
    "    # jars =  [ JARS_DIR + \"/hadoop-aws-3.3.4.jar\", JARS_DIR + \"/aws-java-sdk-bundle-1.12.262.jar\", \n",
    "    #          ]\n",
    "    jars = \"../../jars/hadoop-aws-3.3.4.jar,../../jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "            \n",
    "    builder = SparkSession.builder \\\n",
    "            .appName(\"DeltaConvert\") \\\n",
    "            .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:3.1.0\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", endpoint_url) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "            .config(\"spark.jars\", jars)\n",
    "    \n",
    "    spark = configure_spark_with_delta_pip(\n",
    "        builder,\n",
    "    ).getOrCreate()\n",
    "    \n",
    "    logging.info('Spark session successfully created!')\n",
    "    \n",
    "    client = MinIOClient(\n",
    "        endpoint_url=endpoint_url,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key\n",
    "    )\n",
    "    client.create_bucket(BUCKET_NAME_3)\n",
    "    \n",
    "    # Convert to delta format\n",
    "    for file in client.list_parquet_files(bucket_name=BUCKET_NAME_2):\n",
    "        if \"yellow\" in file:\n",
    "            df = spark.read.parquet(f\"s3a://{BUCKET_NAME_2}/{file}\")\n",
    "            df.write.format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .save(f\"s3a://{BUCKET_NAME_3}/{datalake_cfg[\"folder_name\"]}\")\n",
    "            logging.info(f\"File {file} converted to delta format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_convert(MINIO_ENDPOINT, MINIO_ACCESS_KEY, MINIO_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 15:57:39,559:<module>:INFO:Spark session successfully created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket sandbox already exists, skip creating!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "jars = \"../../jars/hadoop-aws-3.3.4.jar,../../jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "        .appName(\"DeltaConvert\") \\\n",
    "        .config(\"spark.executor.memory\", '2g') \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.jars\", jars)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder,\n",
    "    extra_packages=[\"org.apache.hadoop:hadoop-aws:3.3.4\"]\n",
    ").getOrCreate()\n",
    "\n",
    "logging.info('Spark session successfully created!')\n",
    "\n",
    "client = MinIOClient(\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY\n",
    ")\n",
    "client.create_bucket(BUCKET_NAME_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in client.list_parquet_files(bucket_name=BUCKET_NAME_2):\n",
    "#     df = spark.read.parquet(f\"s3a://{BUCKET_NAME_2}/{file}\")\n",
    "#     df.write.format(\"delta\") \\\n",
    "#             .mode(\"overwrite\") \\\n",
    "#             .save(f\"s3a://{BUCKET_NAME_3}/{datalake_cfg[\"folder_name\"]}\")\n",
    "#     logging.info(f\"File {file} converted to delta format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = client.list_parquet_files(bucket_name=BUCKET_NAME_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = list_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023/yellow_tripdata_2023-01.parquet'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://processed'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"s3://{BUCKET_NAME_2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://processed/2023/yellow_tripdata_2023-01.parquet'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"s3://{BUCKET_NAME_2}/{file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"s3a://{BUCKET_NAME_2}/\" + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "|congestion_surcharge|dolocationid|   dropoff_datetime|  dropoff_latitude|  dropoff_longitude|extra|fare_amount|improvement_surcharge|mta_tax|passenger_count|payment_type|    pickup_datetime|pickup_latitude|  pickup_longitude|pulocationid|ratecodeid|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|vendorid|\n",
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "|                 2.5|         141|2023-01-01 00:40:36|         37.742295|       -122.4651055|  1.0|        9.3|                  1.0|    0.5|            1.0|           2|2023-01-01 00:32:10|     30.3021212|-81.61965224433564|         161|       1.0|                 N|       0.0|         0.0|        14.3|         0.97|       2|\n",
      "|                 2.5|         237|2023-01-01 01:01:27|       -35.0236389|        138.6767413|  1.0|        7.9|                  1.0|    0.5|            1.0|           1|2023-01-01 00:55:08|     40.7827725| -73.9653627406542|          43|       1.0|                 N|       4.0|         0.0|        16.9|          1.1|       2|\n",
      "|                 2.5|         238|2023-01-01 00:37:49|       -35.0235084|        138.6766458|  1.0|       14.9|                  1.0|    0.5|            1.0|           1|2023-01-01 00:25:04|     36.1034126|       -84.1318632|          48|       1.0|                 N|      15.0|         0.0|        34.9|         2.51|       2|\n",
      "|                 0.0|           7|2023-01-01 00:13:25|        46.1882007|       -123.8319802| 7.25|       12.1|                  1.0|    0.5|            0.0|           1|2023-01-01 00:03:48|     40.7757145|-73.87336398511545|         138|       1.0|                 N|       0.0|         0.0|       20.85|          1.9|       1|\n",
      "|                 2.5|          79|2023-01-01 00:21:19|        40.7292688|        -73.9873613|  1.0|       11.4|                  1.0|    0.5|            1.0|           1|2023-01-01 00:10:29|     30.0474239|       -90.6898128|         107|       1.0|                 N|      3.28|         0.0|       19.68|         1.43|       2|\n",
      "|                 2.5|         137|2023-01-01 01:02:52|        40.7395463|        -73.9770832|  1.0|       12.8|                  1.0|    0.5|            1.0|           1|2023-01-01 00:50:34|     30.3021212|-81.61965224433564|         161|       1.0|                 N|      10.0|         0.0|        27.8|         1.84|       2|\n",
      "|                 2.5|         143|2023-01-01 00:19:49|45.449931750000005|-122.72446558106034|  1.0|       12.1|                  1.0|    0.5|            1.0|           1|2023-01-01 00:09:22|     -35.015787|        138.636155|         239|       1.0|                 N|      3.42|         0.0|       20.52|         1.66|       2|\n",
      "|                 2.5|         236|2023-01-01 00:36:40|          -35.0153|          138.63557|  1.0|       17.7|                  1.0|    0.5|            1.0|           1|2023-01-01 00:21:44|     40.7498417|        -73.984251|         164|       1.0|                 N|      5.68|         0.0|       28.38|         2.95|       2|\n",
      "|                 2.5|         107|2023-01-01 00:50:36|        30.0474239|        -90.6898128|  1.0|       14.9|                  1.0|    0.5|            1.0|           2|2023-01-01 00:39:42|      37.742295|      -122.4651055|         141|       1.0|                 N|       0.0|         0.0|        19.9|         3.01|       2|\n",
      "|                 2.5|          68|2023-01-01 01:01:45|       37.92252635|  -96.7615377706732|  1.0|       11.4|                  1.0|    0.5|            1.0|           1|2023-01-01 00:53:01|     40.7360717|       -73.9901888|         234|       1.0|                 N|      3.28|         0.0|       19.68|          1.8|       2|\n",
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733522"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://{BUCKET_NAME_3}/{datalake_cfg[\"folder_name\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('./part-00002-73de2297-9823-488e-90d4-61b970e49e28-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2024-09-09'\n",
    "YEAR = DATE.split('-')[0]\n",
    "MONTH = DATE.split('-')[1]\n",
    "DAY = DATE.split('-')[2]\n",
    "\n",
    "path = f\"s3a://{BUCKET_NAME}/{YEAR}/{TAXI_TYPE}/{MONTH}/{DAY}.parquet\"\n",
    "root_folder = \n",
    "for root, dirs, files in os.walk(root_folder):\n",
    "        print(f'Đang duyệt thư mục: {root}')\n",
    "        \n",
    "        # In ra các thư mục con\n",
    "        if dirs:\n",
    "            print(\"Các thư mục con:\")\n",
    "            for dir in dirs:\n",
    "                print(f\"- {dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "    \n",
    "jars = \"../../../jars/hadoop-aws-3.3.4.jar,../../../jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "        .appName(\"DeltaConvert\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provide+r\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.jars\", jars)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder,\n",
    "    extra_packages=[\"org.apache.hadoop:hadoop-aws:3.3.4\"]\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
