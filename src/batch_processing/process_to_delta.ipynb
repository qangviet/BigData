{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.helpers import load_cfg \n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_cfg(cfg_file):\n",
    "    \"\"\"\n",
    "    Load configuration from a YAML config file\n",
    "    \"\"\"\n",
    "    cfg = None\n",
    "    with open(cfg_file, \"r\") as f:\n",
    "        try:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.minio_utils import MinIOClient\n",
    "\n",
    "from minio import Minio\n",
    "\n",
    "\n",
    "class MinIOClient:\n",
    "    def __init__(self, endpoint_url, access_key, secret_key):\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.access_key = access_key\n",
    "        self.secret_key = secret_key\n",
    "\n",
    "    def create_conn(self):\n",
    "        client = Minio(\n",
    "            endpoint=self.endpoint_url,\n",
    "            access_key=self.access_key,\n",
    "            secret_key=self.secret_key,\n",
    "            secure=False,\n",
    "        )\n",
    "        return client\n",
    "\n",
    "    def create_bucket(self, bucket_name):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # Create bucket if not exist\n",
    "        found = client.bucket_exists(bucket_name=bucket_name)\n",
    "        if not found:\n",
    "            client.make_bucket(bucket_name=bucket_name)\n",
    "            print(f\"Bucket {bucket_name} created successfully!\")\n",
    "        else:\n",
    "            print(f\"Bucket {bucket_name} already exists, skip creating!\")\n",
    "\n",
    "    def list_parquet_files(self, bucket_name, prefix=\"\"):\n",
    "        client = self.create_conn()\n",
    "\n",
    "        # List all objects in the bucket with the given prefix\n",
    "        objects = client.list_objects(bucket_name, prefix=prefix, recursive=True)\n",
    "        # Filter and collect Parquet file names\n",
    "        parquet_files = [\n",
    "            obj.object_name for obj in objects if obj.object_name.endswith(\".parquet\")\n",
    "        ]\n",
    "\n",
    "        return parquet_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s:%(funcName)s:%(levelname)s:%(message)s')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\BigData_2\\MyProject\\src\\batch_processing\n"
     ]
    }
   ],
   "source": [
    "__file__ = os.getcwd()\n",
    "print(__file__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\BigData_2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root = os.path.dirname(\n",
    "    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    ")\n",
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG_FILE = os.path.join(project_root, \"MyProject/config\", \"datalake.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endpoint': 'localhost:9000',\n",
       " 'bucket_name_1': 'raw',\n",
       " 'bucket_name_2': 'processed',\n",
       " 'bucket_name_3': 'sandbox',\n",
       " 'folder_name': 'batch',\n",
       " 'access_key': 'Xs27nx9M4HgPQ5PXZiUE',\n",
       " 'secret_key': '8iifKZlUZh1NRbepsISUMdg1CxlaIC6OSPQk5X59'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = load_cfg(CFG_FILE)\n",
    "datalake_cfg = cfg[\"datalake\"]\n",
    "datalake_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ENDPOINT = datalake_cfg[\"endpoint\"]\n",
    "MINIO_ACCESS_KEY = datalake_cfg[\"access_key\"]\n",
    "MINIO_SECRET_KEY = datalake_cfg[\"secret_key\"]\n",
    "BUCKET_NAME_2 = datalake_cfg['bucket_name_2']\n",
    "BUCKET_NAME_3 = datalake_cfg['bucket_name_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('localhost:9000',\n",
       " 'Xs27nx9M4HgPQ5PXZiUE',\n",
       " '8iifKZlUZh1NRbepsISUMdg1CxlaIC6OSPQk5X59',\n",
       " 'processed',\n",
       " 'sandbox')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MINIO_ENDPOINT, MINIO_ACCESS_KEY, MINIO_SECRET_KEY, BUCKET_NAME_2, BUCKET_NAME_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\BigData_2\\\\MyProject/jars'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JARS_DIR = os.path.join(project_root, \"MyProject/jars\")\n",
    "JARS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jars =  [ JARS_DIR + \"/hadoop-aws-3.3.4.jar\", JARS_DIR + \"/aws-java-sdk-bundle-1.12.262.jar\", \n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\BigData_2\\\\MyProject/jars/hadoop-aws-3.3.4.jar,e:\\\\BigData_2\\\\MyProject/jars/aws-java-sdk-bundle-1.12.262.jar'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join(jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# PySpark\n",
    "###############################################\n",
    "\n",
    "def delta_convert(endpoint_url, access_key, secret_key):\n",
    "    \"\"\"\n",
    "        Convert parquet file to delta format\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    from delta.pip_utils import configure_spark_with_delta_pip\n",
    "    \n",
    "    # jars =  [ JARS_DIR + \"/hadoop-aws-3.3.4.jar\", JARS_DIR + \"/aws-java-sdk-bundle-1.12.262.jar\", \n",
    "    #          ]\n",
    "    jars = \"../../jars/hadoop-aws-3.3.4.jar,../../jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "            \n",
    "    builder = SparkSession.builder \\\n",
    "            .appName(\"DeltaConvert\") \\\n",
    "            .config(\"spark.jars\", jars) \\\n",
    "            .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:3.1.0\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", endpoint_url) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    \n",
    "    spark = configure_spark_with_delta_pip(\n",
    "        builder,\n",
    "    ).getOrCreate()\n",
    "    \n",
    "    logging.info('Spark session successfully created!')\n",
    "    \n",
    "    client = MinIOClient(\n",
    "        endpoint_url=endpoint_url,\n",
    "        access_key=access_key,\n",
    "        secret_key=secret_key\n",
    "    )\n",
    "    client.create_bucket(BUCKET_NAME_3)\n",
    "    \n",
    "    # Convert to delta format\n",
    "    for file in client.list_parquet_files(bucket_name=BUCKET_NAME_2):\n",
    "        if \"yellow\" in file:\n",
    "            df = spark.read.parquet(f\"s3a://{BUCKET_NAME_2}/{file}\")\n",
    "            df.write.format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .save(f\"s3a://{BUCKET_NAME_3}/{datalake_cfg[\"folder_name\"]}\")\n",
    "            logging.info(f\"File {file} converted to delta format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 22:57:44,042:delta_convert:INFO:Spark session successfully created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket sandbox already exists, skip creating!\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'dolocationid' and 'dolocationid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdelta_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMINIO_ENDPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMINIO_ACCESS_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMINIO_SECRET_KEY\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 50\u001b[0m, in \u001b[0;36mdelta_convert\u001b[1;34m(endpoint_url, access_key, secret_key)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[0;32m     47\u001b[0m     df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3a://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_NAME_2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 50\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBUCKET_NAME_3\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatalake_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfolder_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m converted to delta format!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\BigData_2\\env\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\BigData_2\\env\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\BigData_2\\env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'dolocationid' and 'dolocationid'"
     ]
    }
   ],
   "source": [
    "delta_convert(MINIO_ENDPOINT, MINIO_ACCESS_KEY, MINIO_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 22:58:11,755:<module>:INFO:Spark session successfully created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket sandbox already exists, skip creating!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "jars = \"../../jars/hadoop-aws-3.3.4.jar,../../jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "        .appName(\"DeltaConvert\") \\\n",
    "        .config(\"spark.executor.memory\", '2g') \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.jars\", jars)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder,\n",
    "    extra_packages=[\"org.apache.hadoop:hadoop-aws:3.3.4\"]\n",
    ").getOrCreate()\n",
    "\n",
    "logging.info('Spark session successfully created!')\n",
    "\n",
    "client = MinIOClient(\n",
    "    endpoint_url=MINIO_ENDPOINT,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY\n",
    ")\n",
    "client.create_bucket(BUCKET_NAME_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in client.list_parquet_files(bucket_name=BUCKET_NAME_2):\n",
    "#     df = spark.read.parquet(f\"s3a://{BUCKET_NAME_2}/{file}\")\n",
    "#     df.write.format(\"delta\") \\\n",
    "#             .mode(\"overwrite\") \\\n",
    "#             .save(f\"s3a://{BUCKET_NAME_3}/{datalake_cfg[\"folder_name\"]}\")\n",
    "#     logging.info(f\"File {file} converted to delta format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = client.list_parquet_files(bucket_name=BUCKET_NAME_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = list_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023/yellow_tripdata_2023-01.parquet'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://processed'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"s3://{BUCKET_NAME_2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://processed/2023/yellow_tripdata_2023-01.parquet'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"s3://{BUCKET_NAME_2}/{file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"s3a://{BUCKET_NAME_2}/\" + file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "|congestion_surcharge|dolocationid|   dropoff_datetime|  dropoff_latitude|  dropoff_longitude|extra|fare_amount|            id|id_customer|improvement_surcharge|mta_tax|passenger_count|payment_type|    pickup_datetime|pickup_latitude|  pickup_longitude|pulocationid|rate|ratecodeid|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|vendorid|\n",
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "|                 2.5|         141|2023-01-01 00:40:36|         37.742295|       -122.4651055|  1.0|        9.3|20230100000001|     310976|                  1.0|    0.5|            1.0|           2|2023-01-01 00:32:10|     30.3021212|-81.61965224433564|         161| 4.5|       1.0|                 N|       0.0|         0.0|        14.3|         0.97|       2|\n",
      "|                 2.5|         237|2023-01-01 01:01:27|       -35.0236389|        138.6767413|  1.0|        7.9|20230100000002|     218354|                  1.0|    0.5|            1.0|           1|2023-01-01 00:55:08|     40.7827725| -73.9653627406542|          43| 2.1|       1.0|                 N|       4.0|         0.0|        16.9|          1.1|       2|\n",
      "|                 2.5|         238|2023-01-01 00:37:49|       -35.0235084|        138.6766458|  1.0|       14.9|20230100000003|     463540|                  1.0|    0.5|            1.0|           1|2023-01-01 00:25:04|     36.1034126|       -84.1318632|          48| 4.1|       1.0|                 N|      15.0|         0.0|        34.9|         2.51|       2|\n",
      "|                 0.0|           7|2023-01-01 00:13:25|        46.1882007|       -123.8319802| 7.25|       12.1|20230100000004|     649163|                  1.0|    0.5|            0.0|           1|2023-01-01 00:03:48|     40.7757145|-73.87336398511545|         138| 2.4|       1.0|                 N|       0.0|         0.0|       20.85|          1.9|       1|\n",
      "|                 2.5|          79|2023-01-01 00:21:19|        40.7292688|        -73.9873613|  1.0|       11.4|20230100000005|     269214|                  1.0|    0.5|            1.0|           1|2023-01-01 00:10:29|     30.0474239|       -90.6898128|         107| 3.6|       1.0|                 N|      3.28|         0.0|       19.68|         1.43|       2|\n",
      "|                 2.5|         137|2023-01-01 01:02:52|        40.7395463|        -73.9770832|  1.0|       12.8|20230100000006|     528386|                  1.0|    0.5|            1.0|           1|2023-01-01 00:50:34|     30.3021212|-81.61965224433564|         161| 1.4|       1.0|                 N|      10.0|         0.0|        27.8|         1.84|       2|\n",
      "|                 2.5|         143|2023-01-01 00:19:49|45.449931750000005|-122.72446558106034|  1.0|       12.1|20230100000007|     507579|                  1.0|    0.5|            1.0|           1|2023-01-01 00:09:22|     -35.015787|        138.636155|         239| 2.6|       1.0|                 N|      3.42|         0.0|       20.52|         1.66|       2|\n",
      "|                 2.5|         236|2023-01-01 00:36:40|          -35.0153|          138.63557|  1.0|       17.7|20230100000009|     751471|                  1.0|    0.5|            1.0|           1|2023-01-01 00:21:44|     40.7498417|        -73.984251|         164| 3.2|       1.0|                 N|      5.68|         0.0|       28.38|         2.95|       2|\n",
      "|                 2.5|         107|2023-01-01 00:50:36|        30.0474239|        -90.6898128|  1.0|       14.9|20230100000010|     659631|                  1.0|    0.5|            1.0|           2|2023-01-01 00:39:42|      37.742295|      -122.4651055|         141| 3.8|       1.0|                 N|       0.0|         0.0|        19.9|         3.01|       2|\n",
      "|                 2.5|          68|2023-01-01 01:01:45|       37.92252635|  -96.7615377706732|  1.0|       11.4|20230100000011|     346184|                  1.0|    0.5|            1.0|           1|2023-01-01 00:53:01|     40.7360717|       -73.9901888|         234| 1.7|       1.0|                 N|      3.28|         0.0|       19.68|          1.8|       2|\n",
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733522"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(f\"s3a://{BUCKET_NAME_3}/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = spark.read.format(\"delta\") \\\n",
    "                .load(f\"s3a://{BUCKET_NAME_3}/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+----------------+------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+------------------+-------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "|congestion_surcharge|dolocationid|   dropoff_datetime|dropoff_latitude| dropoff_longitude|extra|fare_amount|            id|id_customer|improvement_surcharge|mta_tax|passenger_count|payment_type|    pickup_datetime|   pickup_latitude|   pickup_longitude|pulocationid|rate|ratecodeid|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|vendorid|\n",
      "+--------------------+------------+-------------------+----------------+------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+------------------+-------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "|                 2.5|          68|2023-01-13 16:32:30|     37.92252635| -96.7615377706732|  2.5|        8.6|20230101149258|     151337|                  1.0|    0.5|            1.0|           1|2023-01-13 16:27:05|       37.91593655| -96.78585964406288|         246| 2.4|       1.0|                 N|      3.02|         0.0|       18.12|         1.24|       2|\n",
      "|                 2.5|         161|2023-01-13 16:49:33|      30.3021212|-81.61965224433564|  2.5|       14.9|20230101149259|     974436|                  1.0|    0.5|            2.0|           1|2023-01-13 16:33:49|       37.92252635|  -96.7615377706732|          68| 5.0|       1.0|                 N|      4.28|         0.0|       25.68|          2.1|       2|\n",
      "|                 2.5|          68|2023-01-13 16:15:55|     37.92252635| -96.7615377706732|  5.0|       13.5|20230101149260|     218916|                  1.0|    0.5|            1.0|           2|2023-01-13 16:01:09|        35.8631305| -78.63670218521418|         163| 2.3|       1.0|                 N|       0.0|         0.0|        20.0|          1.7|       1|\n",
      "|                 2.5|         230|2023-01-13 16:27:27|      40.7572614|-73.98589982948505|  5.0|       10.0|20230101149261|     668237|                  1.0|    0.5|            1.0|           1|2023-01-13 16:16:42|       37.92252635|  -96.7615377706732|          68| 1.9|       1.0|                 N|      4.15|         0.0|       20.65|          1.5|       1|\n",
      "|                 2.5|         238|2023-01-13 17:04:39|     -35.0235084|       138.6766458|  2.5|        9.3|20230101149263|     879536|                  1.0|    0.5|            2.0|           1|2023-01-13 16:57:29|45.449931750000005|-122.72446558106034|         143| 3.1|       1.0|                 N|      3.16|         0.0|       18.96|         1.09|       2|\n",
      "|                 2.5|         186|2023-01-13 16:21:25|      40.7512885|       -73.9903622|  5.0|        5.1|20230101149264|     695974|                  1.0|    0.5|            1.0|           2|2023-01-13 16:17:17|        40.7360717|        -73.9901888|         234| 4.9|       1.0|                 N|       0.0|         0.0|        11.6|          0.4|       1|\n",
      "|                 2.5|         234|2023-01-13 16:40:16|      40.7360717|       -73.9901888|  5.0|        9.3|20230101149265|     511427|                  1.0|    0.5|            1.0|           1|2023-01-13 16:30:27|        30.3021212| -81.61965224433564|         161| 2.6|       1.0|                 N|      3.95|         0.0|       19.75|          1.1|       1|\n",
      "|                 2.5|         234|2023-01-13 16:50:55|      40.7360717|       -73.9901888|  2.5|        7.2|20230101149266|     881690|                  1.0|    0.5|            1.0|           2|2023-01-13 16:45:55|        40.7498417|         -73.984251|         164| 1.6|       1.0|                 N|       0.0|         0.0|        13.7|         0.98|       2|\n",
      "|                 2.5|         161|2023-01-13 17:09:52|      30.3021212|-81.61965224433564|  5.0|       12.1|20230101149267|     688018|                  1.0|    0.5|            1.0|           2|2023-01-13 16:58:14|       37.92252635|  -96.7615377706732|          68| 4.0|       1.0|                 N|       0.0|         0.0|        18.6|          1.5|       1|\n",
      "|                 2.5|         236|2023-01-13 16:59:45|        -35.0153|         138.63557|  5.0|        7.2|20230101149268|     510622|                  1.0|    0.5|            1.0|           1|2023-01-13 16:52:45|       -35.0236389|        138.6767413|         237| 1.8|       1.0|                 N|      2.75|         0.0|       16.45|          0.9|       1|\n",
      "+--------------------+------------+-------------------+----------------+------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+------------------+-------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"s3a://{BUCKET_NAME_3}/test_2/test_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_2 = spark.read.format(\"parquet\") \\\n",
    "                .load(f\"s3a://{BUCKET_NAME_3}/test_2/test_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "|congestion_surcharge|dolocationid|   dropoff_datetime|  dropoff_latitude|  dropoff_longitude|extra|fare_amount|            id|id_customer|improvement_surcharge|mta_tax|passenger_count|payment_type|    pickup_datetime|pickup_latitude|  pickup_longitude|pulocationid|rate|ratecodeid|store_and_fwd_flag|tip_amount|tolls_amount|total_amount|trip_distance|vendorid|\n",
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "|                 2.5|         141|2023-01-01 00:40:36|         37.742295|       -122.4651055|  1.0|        9.3|20230100000001|     310976|                  1.0|    0.5|            1.0|           2|2023-01-01 00:32:10|     30.3021212|-81.61965224433564|         161| 4.5|       1.0|                 N|       0.0|         0.0|        14.3|         0.97|       2|\n",
      "|                 2.5|         237|2023-01-01 01:01:27|       -35.0236389|        138.6767413|  1.0|        7.9|20230100000002|     218354|                  1.0|    0.5|            1.0|           1|2023-01-01 00:55:08|     40.7827725| -73.9653627406542|          43| 2.1|       1.0|                 N|       4.0|         0.0|        16.9|          1.1|       2|\n",
      "|                 2.5|         238|2023-01-01 00:37:49|       -35.0235084|        138.6766458|  1.0|       14.9|20230100000003|     463540|                  1.0|    0.5|            1.0|           1|2023-01-01 00:25:04|     36.1034126|       -84.1318632|          48| 4.1|       1.0|                 N|      15.0|         0.0|        34.9|         2.51|       2|\n",
      "|                 0.0|           7|2023-01-01 00:13:25|        46.1882007|       -123.8319802| 7.25|       12.1|20230100000004|     649163|                  1.0|    0.5|            0.0|           1|2023-01-01 00:03:48|     40.7757145|-73.87336398511545|         138| 2.4|       1.0|                 N|       0.0|         0.0|       20.85|          1.9|       1|\n",
      "|                 2.5|          79|2023-01-01 00:21:19|        40.7292688|        -73.9873613|  1.0|       11.4|20230100000005|     269214|                  1.0|    0.5|            1.0|           1|2023-01-01 00:10:29|     30.0474239|       -90.6898128|         107| 3.6|       1.0|                 N|      3.28|         0.0|       19.68|         1.43|       2|\n",
      "|                 2.5|         137|2023-01-01 01:02:52|        40.7395463|        -73.9770832|  1.0|       12.8|20230100000006|     528386|                  1.0|    0.5|            1.0|           1|2023-01-01 00:50:34|     30.3021212|-81.61965224433564|         161| 1.4|       1.0|                 N|      10.0|         0.0|        27.8|         1.84|       2|\n",
      "|                 2.5|         143|2023-01-01 00:19:49|45.449931750000005|-122.72446558106034|  1.0|       12.1|20230100000007|     507579|                  1.0|    0.5|            1.0|           1|2023-01-01 00:09:22|     -35.015787|        138.636155|         239| 2.6|       1.0|                 N|      3.42|         0.0|       20.52|         1.66|       2|\n",
      "|                 2.5|         236|2023-01-01 00:36:40|          -35.0153|          138.63557|  1.0|       17.7|20230100000009|     751471|                  1.0|    0.5|            1.0|           1|2023-01-01 00:21:44|     40.7498417|        -73.984251|         164| 3.2|       1.0|                 N|      5.68|         0.0|       28.38|         2.95|       2|\n",
      "|                 2.5|         107|2023-01-01 00:50:36|        30.0474239|        -90.6898128|  1.0|       14.9|20230100000010|     659631|                  1.0|    0.5|            1.0|           2|2023-01-01 00:39:42|      37.742295|      -122.4651055|         141| 3.8|       1.0|                 N|       0.0|         0.0|        19.9|         3.01|       2|\n",
      "|                 2.5|          68|2023-01-01 01:01:45|       37.92252635|  -96.7615377706732|  1.0|       11.4|20230100000011|     346184|                  1.0|    0.5|            1.0|           1|2023-01-01 00:53:01|     40.7360717|       -73.9901888|         234| 1.7|       1.0|                 N|      3.28|         0.0|       19.68|          1.8|       2|\n",
      "+--------------------+------------+-------------------+------------------+-------------------+-----+-----------+--------------+-----------+---------------------+-------+---------------+------------+-------------------+---------------+------------------+------------+----+----------+------------------+----------+------------+------------+-------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df_2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2733522"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_2.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.pip_utils import configure_spark_with_delta_pip\n",
    "    \n",
    "jars = \"../../../jars/hadoop-aws-3.3.4.jar,../../../jars/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "        .appName(\"DeltaConvert\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provide+r\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.jars\", jars)\n",
    "\n",
    "spark = configure_spark_with_delta_pip(\n",
    "    builder,\n",
    "    extra_packages=[\"org.apache.hadoop:hadoop-aws:3.3.4\"]\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
